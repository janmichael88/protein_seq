{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "* For this notebook we will be exploring other stuctures to classify proteins from their sequence\n",
    "* We will explore 4 different ways\n",
    "* The first way, we will try changing some of the architecture using the emebedding and CONV1d\n",
    "* The second way, we ill trying implemeting an LSTM after the embedding layer\n",
    "* The third way, we will try implementing a GRU after the embedding layer\n",
    "* The fourth way, we will try implementing bi bi-diertional LSTMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.4'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras \n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "proteins = pd.read_csv(\"top_15_proteins.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sequence</th>\n",
       "      <th>classification</th>\n",
       "      <th>num_residues</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>67</td>\n",
       "      <td>TYTTRQIGAKNTLEYKVYIEKDGKPVSAFHDIPLYADKENNIFNMV...</td>\n",
       "      <td>HYDROLASE</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>68</td>\n",
       "      <td>TYTTRQIGAKNTLEYKVYIEKDGKPVSAFHDIPLYADKENNIFNMV...</td>\n",
       "      <td>HYDROLASE</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>74</td>\n",
       "      <td>MKTAYIAKQRQISFVKSHFSRQLEERLGLIEVQAPILSRVGDGTQD...</td>\n",
       "      <td>LIGASE</td>\n",
       "      <td>330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75</td>\n",
       "      <td>MKTAYIAKQRQISFVKSHFSRQLEERLGLIEVQAPILSRVGDGTQD...</td>\n",
       "      <td>LIGASE</td>\n",
       "      <td>330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>76</td>\n",
       "      <td>KESAAAKFERQHMDSGNSPSSSSNYCNLMMCCRKMTQGKCKPVNTF...</td>\n",
       "      <td>HYDROLASE</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           sequence  \\\n",
       "0          67  TYTTRQIGAKNTLEYKVYIEKDGKPVSAFHDIPLYADKENNIFNMV...   \n",
       "1          68  TYTTRQIGAKNTLEYKVYIEKDGKPVSAFHDIPLYADKENNIFNMV...   \n",
       "2          74  MKTAYIAKQRQISFVKSHFSRQLEERLGLIEVQAPILSRVGDGTQD...   \n",
       "3          75  MKTAYIAKQRQISFVKSHFSRQLEERLGLIEVQAPILSRVGDGTQD...   \n",
       "4          76  KESAAAKFERQHMDSGNSPSSSSNYCNLMMCCRKMTQGKCKPVNTF...   \n",
       "\n",
       "  classification  num_residues  \n",
       "0      HYDROLASE           286  \n",
       "1      HYDROLASE           286  \n",
       "2         LIGASE           330  \n",
       "3         LIGASE           330  \n",
       "4      HYDROLASE           124  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proteins.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(283101, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proteins.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tranform the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# Transform labels to one-hot\n",
    "lb = LabelBinarizer()\n",
    "Y = lb.fit_transform(proteins['classification'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['HYDROLASE', 'HYDROLASE/HYDROLASE INHIBITOR', 'IMMUNE SYSTEM',\n",
       "       'ISOMERASE', 'LIGASE', 'LYASE', 'OXIDOREDUCTASE', 'RIBOSOME',\n",
       "       'RIBOSOME/ANTIBIOTIC', 'SIGNALING PROTEIN', 'TRANSCRIPTION',\n",
       "       'TRANSFERASE', 'TRANSPORT PROTEIN', 'VIRAL PROTEIN', 'VIRUS'],\n",
       "      dtype='<U29')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[34]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize the Sequences\n",
    "* This time try adding padding pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import text, sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# maximum length of sequence, everything afterwards is discarded!\n",
    "max_length = 512\n",
    "\n",
    "#create and fit tokenizer\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(proteins['sequence'])\n",
    "#represent input data as word rank number sequences\n",
    "X = tokenizer.texts_to_sequences(proteins['sequence'])\n",
    "X = sequence.pad_sequences(X, maxlen=max_length,padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'l': 271251,\n",
       "             'f': 263330,\n",
       "             'w': 218243,\n",
       "             'e': 267895,\n",
       "             'v': 269066,\n",
       "             's': 268119,\n",
       "             'n': 265583,\n",
       "             'g': 276371,\n",
       "             'q': 264838,\n",
       "             't': 269758,\n",
       "             'c': 213007,\n",
       "             'd': 266266,\n",
       "             'a': 277393,\n",
       "             'r': 270110,\n",
       "             'i': 267264,\n",
       "             'y': 262657,\n",
       "             'k': 270045,\n",
       "             'p': 267751,\n",
       "             'm': 258030,\n",
       "             'h': 258236,\n",
       "             'u': 5915,\n",
       "             'x': 4954,\n",
       "             'z': 3,\n",
       "             'b': 4,\n",
       "             'o': 2})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  2,  6,  2, 15,  4, 11,  6,  6, 12,  2,  9, 21, 15,  3, 11, 17,\n",
       "        8, 18,  3,  5,  2,  7,  4,  9,  3,  4,  1,  4, 18,  4,  1,  6,  2,\n",
       "       16,  9,  5,  1,  5,  4,  9, 12,  1,  5,  8,  2, 15,  5,  8,  1, 16,\n",
       "       14,  3,  3,  7,  3,  1,  2, 10, 21, 12,  4,  7,  8,  9, 18,  8, 11,\n",
       "       13,  2,  6, 13, 14,  8,  2,  1,  8,  4, 10,  1,  1, 17, 11, 21,  1,\n",
       "        2,  9,  7, 15,  5, 11,  2,  9, 12, 16, 13, 12, 15,  6, 15,  2,  4,\n",
       "        4,  5,  6, 19, 13,  7,  5,  3,  7,  7,  9,  9,  2, 15,  4, 10, 11,\n",
       "       15,  1,  5, 18,  3,  7,  8,  1,  4, 15, 19,  1,  4, 14,  9, 18, 13,\n",
       "        7,  7, 10,  7,  2,  2,  9,  2,  2, 16,  6,  1,  2,  5, 10,  9,  4,\n",
       "       11,  9,  9,  1,  8, 11,  9, 15,  8,  7,  5,  3, 15,  7, 15,  9,  8,\n",
       "        7,  9, 15, 13, 14, 10,  4, 16, 16, 10, 11,  6, 10, 13,  6,  3, 21,\n",
       "        7,  2, 12,  1,  7,  3,  3, 21,  7,  2,  5,  2,  1,  4,  4,  9, 15,\n",
       "       11, 13,  6, 11,  9,  7,  4,  4, 12, 10, 10,  7,  1,  7,  9,  9, 10,\n",
       "       11, 16,  2,  7, 15, 19,  1,  2, 11, 11,  6,  4,  1,  6, 10, 15, 11,\n",
       "        5, 11], dtype=int32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[4545]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Model\n",
    "* Change embedding size 16\n",
    "* Adding another conv1d and max pooling layer (with 16 filters and kernel size of 2)\n",
    "* Now try 50 epochs this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-12524c094adb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# create the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "embedding_dim = 16\n",
    "\n",
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(tokenizer.word_index)+1, embedding_dim, input_length=max_length))\n",
    "model.add(Conv1D(filters=64, kernel_size=8, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=16, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(15, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((283101, 512), (283101, 15))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape,Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep X_test and y_test until the end get valids\n",
    "X_test.shape\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 229311 samples, validate on 25479 samples\n",
      "Epoch 1/50\n",
      "229311/229311 [==============================] - 397s 2ms/step - loss: 1.3295 - acc: 0.5858 - val_loss: 0.9753 - val_acc: 0.7080\n",
      "Epoch 2/50\n",
      "229311/229311 [==============================] - 396s 2ms/step - loss: 0.8175 - acc: 0.7577 - val_loss: 0.7526 - val_acc: 0.7817\n",
      "Epoch 3/50\n",
      "229311/229311 [==============================] - 404s 2ms/step - loss: 0.6324 - acc: 0.8136 - val_loss: 0.6471 - val_acc: 0.8164\n",
      "Epoch 4/50\n",
      "229311/229311 [==============================] - 397s 2ms/step - loss: 0.5295 - acc: 0.8439 - val_loss: 0.5711 - val_acc: 0.8372\n",
      "Epoch 5/50\n",
      "229311/229311 [==============================] - 399s 2ms/step - loss: 0.4618 - acc: 0.8627 - val_loss: 0.5386 - val_acc: 0.8513\n",
      "Epoch 6/50\n",
      "229311/229311 [==============================] - 405s 2ms/step - loss: 0.4155 - acc: 0.8767 - val_loss: 0.5241 - val_acc: 0.8565\n",
      "Epoch 7/50\n",
      "229311/229311 [==============================] - 410s 2ms/step - loss: 0.3802 - acc: 0.8853 - val_loss: 0.5034 - val_acc: 0.8641\n",
      "Epoch 8/50\n",
      "229311/229311 [==============================] - 420s 2ms/step - loss: 0.3536 - acc: 0.8928 - val_loss: 0.4771 - val_acc: 0.8723\n",
      "Epoch 9/50\n",
      "229311/229311 [==============================] - 455s 2ms/step - loss: 0.3311 - acc: 0.8985 - val_loss: 0.4787 - val_acc: 0.8763\n",
      "Epoch 10/50\n",
      "229311/229311 [==============================] - 410s 2ms/step - loss: 0.3152 - acc: 0.9027 - val_loss: 0.4920 - val_acc: 0.8743\n",
      "Epoch 11/50\n",
      "229311/229311 [==============================] - 400s 2ms/step - loss: 0.2996 - acc: 0.9071 - val_loss: 0.4691 - val_acc: 0.8826\n",
      "Epoch 12/50\n",
      "229311/229311 [==============================] - 402s 2ms/step - loss: 0.2862 - acc: 0.9105 - val_loss: 0.4634 - val_acc: 0.8861\n",
      "Epoch 13/50\n",
      "229311/229311 [==============================] - 402s 2ms/step - loss: 0.2738 - acc: 0.9146 - val_loss: 0.4573 - val_acc: 0.8882\n",
      "Epoch 14/50\n",
      "229311/229311 [==============================] - 396s 2ms/step - loss: 0.2641 - acc: 0.9165 - val_loss: 0.4489 - val_acc: 0.8929\n",
      "Epoch 15/50\n",
      "229311/229311 [==============================] - 381s 2ms/step - loss: 0.2563 - acc: 0.9190 - val_loss: 0.4468 - val_acc: 0.8939\n",
      "Epoch 16/50\n",
      "229311/229311 [==============================] - 402s 2ms/step - loss: 0.2501 - acc: 0.9208 - val_loss: 0.4506 - val_acc: 0.8948\n",
      "Epoch 17/50\n",
      "229311/229311 [==============================] - 401s 2ms/step - loss: 0.2423 - acc: 0.9227 - val_loss: 0.4664 - val_acc: 0.8914\n",
      "Epoch 18/50\n",
      "229311/229311 [==============================] - 400s 2ms/step - loss: 0.2368 - acc: 0.9242 - val_loss: 0.4562 - val_acc: 0.8953\n",
      "Epoch 19/50\n",
      "229311/229311 [==============================] - 403s 2ms/step - loss: 0.2312 - acc: 0.9264 - val_loss: 0.4535 - val_acc: 0.8989\n",
      "Epoch 20/50\n",
      "229311/229311 [==============================] - 401s 2ms/step - loss: 0.2269 - acc: 0.9271 - val_loss: 0.4575 - val_acc: 0.9004\n",
      "Epoch 21/50\n",
      "229311/229311 [==============================] - 401s 2ms/step - loss: 0.2227 - acc: 0.9289 - val_loss: 0.4579 - val_acc: 0.9001\n",
      "Epoch 22/50\n",
      "229311/229311 [==============================] - 400s 2ms/step - loss: 0.2181 - acc: 0.9298 - val_loss: 0.4508 - val_acc: 0.9015\n",
      "Epoch 23/50\n",
      "229311/229311 [==============================] - 402s 2ms/step - loss: 0.2135 - acc: 0.9309 - val_loss: 0.4515 - val_acc: 0.9022\n",
      "Epoch 24/50\n",
      "229311/229311 [==============================] - 386s 2ms/step - loss: 0.2095 - acc: 0.9320 - val_loss: 0.4614 - val_acc: 0.9026\n",
      "Epoch 25/50\n",
      "229311/229311 [==============================] - 390s 2ms/step - loss: 0.2079 - acc: 0.9331 - val_loss: 0.4617 - val_acc: 0.9013\n",
      "Epoch 26/50\n",
      "229311/229311 [==============================] - 407s 2ms/step - loss: 0.2049 - acc: 0.9331 - val_loss: 0.4693 - val_acc: 0.9033\n",
      "Epoch 27/50\n",
      "229311/229311 [==============================] - 402s 2ms/step - loss: 0.2013 - acc: 0.9345 - val_loss: 0.4575 - val_acc: 0.9040\n",
      "Epoch 28/50\n",
      "229311/229311 [==============================] - 398s 2ms/step - loss: 0.1999 - acc: 0.9352 - val_loss: 0.4684 - val_acc: 0.9040\n",
      "Epoch 29/50\n",
      "229311/229311 [==============================] - 401s 2ms/step - loss: 0.1971 - acc: 0.9358 - val_loss: 0.4628 - val_acc: 0.9076\n",
      "Epoch 30/50\n",
      "229311/229311 [==============================] - 403s 2ms/step - loss: 0.1942 - acc: 0.9365 - val_loss: 0.4717 - val_acc: 0.9036\n",
      "Epoch 31/50\n",
      "229311/229311 [==============================] - 406s 2ms/step - loss: 0.1937 - acc: 0.9365 - val_loss: 0.4648 - val_acc: 0.9066\n",
      "Epoch 32/50\n",
      "229311/229311 [==============================] - 387s 2ms/step - loss: 0.1930 - acc: 0.9369 - val_loss: 0.4651 - val_acc: 0.9038\n",
      "Epoch 33/50\n",
      "229311/229311 [==============================] - 383s 2ms/step - loss: 0.1880 - acc: 0.9384 - val_loss: 0.4718 - val_acc: 0.9049\n",
      "Epoch 34/50\n",
      "229311/229311 [==============================] - 380s 2ms/step - loss: 0.1897 - acc: 0.9379 - val_loss: 0.4710 - val_acc: 0.9049\n",
      "Epoch 35/50\n",
      "229311/229311 [==============================] - 379s 2ms/step - loss: 0.1860 - acc: 0.9389 - val_loss: 0.4780 - val_acc: 0.9062\n",
      "Epoch 36/50\n",
      "229311/229311 [==============================] - 379s 2ms/step - loss: 0.1849 - acc: 0.9394 - val_loss: 0.4896 - val_acc: 0.9010\n",
      "Epoch 37/50\n",
      "229311/229311 [==============================] - 378s 2ms/step - loss: 0.1843 - acc: 0.9394 - val_loss: 0.4681 - val_acc: 0.9084\n",
      "Epoch 38/50\n",
      "229311/229311 [==============================] - 378s 2ms/step - loss: 0.1808 - acc: 0.9403 - val_loss: 0.4766 - val_acc: 0.9069\n",
      "Epoch 39/50\n",
      "229311/229311 [==============================] - 380s 2ms/step - loss: 0.1801 - acc: 0.9407 - val_loss: 0.4731 - val_acc: 0.9082\n",
      "Epoch 40/50\n",
      "229311/229311 [==============================] - 380s 2ms/step - loss: 0.1781 - acc: 0.9413 - val_loss: 0.4809 - val_acc: 0.9066\n",
      "Epoch 41/50\n",
      "229311/229311 [==============================] - 379s 2ms/step - loss: 0.1791 - acc: 0.9410 - val_loss: 0.4667 - val_acc: 0.9093\n",
      "Epoch 42/50\n",
      "229311/229311 [==============================] - 400s 2ms/step - loss: 0.1770 - acc: 0.9419 - val_loss: 0.4798 - val_acc: 0.9055\n",
      "Epoch 43/50\n",
      "229311/229311 [==============================] - 388s 2ms/step - loss: 0.1768 - acc: 0.9411 - val_loss: 0.4822 - val_acc: 0.9077\n",
      "Epoch 44/50\n",
      "229311/229311 [==============================] - 388s 2ms/step - loss: 0.1753 - acc: 0.9421 - val_loss: 0.4835 - val_acc: 0.9074\n",
      "Epoch 45/50\n",
      "229311/229311 [==============================] - 387s 2ms/step - loss: 0.1746 - acc: 0.9422 - val_loss: 0.4709 - val_acc: 0.9113\n",
      "Epoch 46/50\n",
      "229311/229311 [==============================] - 394s 2ms/step - loss: 0.1732 - acc: 0.9424 - val_loss: 0.4852 - val_acc: 0.9089\n",
      "Epoch 47/50\n",
      "229311/229311 [==============================] - 380s 2ms/step - loss: 0.1714 - acc: 0.9432 - val_loss: 0.4786 - val_acc: 0.9085\n",
      "Epoch 48/50\n",
      "229311/229311 [==============================] - 381s 2ms/step - loss: 0.1708 - acc: 0.9430 - val_loss: 0.4889 - val_acc: 0.9067\n",
      "Epoch 49/50\n",
      "229311/229311 [==============================] - 384s 2ms/step - loss: 0.1700 - acc: 0.9438 - val_loss: 0.4680 - val_acc: 0.9115\n",
      "Epoch 50/50\n",
      "229311/229311 [==============================] - 391s 2ms/step - loss: 0.1700 - acc: 0.9432 - val_loss: 0.4700 - val_acc: 0.9101\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c963975f8>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, \n",
    "          validation_data=(X_valid, y_valid), \n",
    "          epochs=50, \n",
    "          batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the the models and histories  to examine later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(model.history.history).to_csv('Protein_Classification_Parameter_Tuning_Other_Architectures/ThreeConvLayersHistory.csv')\n",
    "                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Protein_Classification_Parameter_Tuning_Other_Architectures/ThreeConvLayers.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second  Model\n",
    "* Use and LSTM layer after the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (None, 512, 16)           416       \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 16)                2112      \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 15)                255       \n",
      "=================================================================\n",
      "Total params: 2,783\n",
      "Trainable params: 2,783\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM\n",
    "\n",
    "embedding_dim = 16\n",
    "depth  = 16\n",
    "\n",
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(tokenizer.word_index)+1, embedding_dim, input_length=max_length))\n",
    "model.add(LSTM(depth))\n",
    "model.add(Dense(15, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note this takes quite a long time for training. Use a smaller number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 229311 samples, validate on 25479 samples\n",
      "Epoch 1/15\n",
      "229311/229311 [==============================] - 1646s 7ms/step - loss: 2.0589 - acc: 0.3382 - val_loss: 1.9320 - val_acc: 0.3604\n",
      "Epoch 2/15\n",
      "229311/229311 [==============================] - 1649s 7ms/step - loss: 1.9854 - acc: 0.3452 - val_loss: 1.9482 - val_acc: 0.3539\n",
      "Epoch 3/15\n",
      "229311/229311 [==============================] - 1625s 7ms/step - loss: 1.8543 - acc: 0.3877 - val_loss: 1.7856 - val_acc: 0.3983\n",
      "Epoch 4/15\n",
      "229311/229311 [==============================] - 1831s 8ms/step - loss: 1.7450 - acc: 0.4146 - val_loss: 1.6687 - val_acc: 0.4310\n",
      "Epoch 5/15\n",
      "229311/229311 [==============================] - 1656s 7ms/step - loss: 1.6560 - acc: 0.4444 - val_loss: 1.6301 - val_acc: 0.4451\n",
      "Epoch 6/15\n",
      "229311/229311 [==============================] - 1623s 7ms/step - loss: 1.6259 - acc: 0.4515 - val_loss: 1.5857 - val_acc: 0.4601\n",
      "Epoch 7/15\n",
      "229311/229311 [==============================] - 1675s 7ms/step - loss: 1.5930 - acc: 0.4626 - val_loss: 1.5640 - val_acc: 0.4666\n",
      "Epoch 8/15\n",
      " 85120/229311 [==========>...................] - ETA: 16:42 - loss: 1.5822 - acc: 0.4634"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, \n",
    "          validation_data=(X_valid, y_valid), \n",
    "          epochs=15, \n",
    "          batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
